# SparK Configuration
# Reference: Tian et al. 2023 "Designing BERT for Convolutional Networks:
#            Sparse and Hierarchical Masked Modeling" (ICLR 2023)
#
# Key advantage over MAE for this project:
#   - Uses ResNet50 backbone, same as MoCo/DINO/BarlowTwins → fair comparison
#   - Convolutional inductive biases suit chest X-rays (consistent anatomy,
#     translation equivariance, hierarchical texture/structure features)
#   - Works well at smaller dataset scales than ViT-based MAE

spark:
  encoder: resnet50
  patch_size: 32           # 224/32 = 7 patches/side → 49 patches total
                           # Each 7×7 position in layer4 = one 32×32 patch (perfect alignment)
  mask_ratio: 0.60         # Mask 60% of patches — more conservative than MAE's 75%
                           # because CNN information leakage makes the task slightly easier
  dec_dim: 256             # Decoder hidden width (lightweight — discarded after pretraining)
  norm_pix_loss: true      # Normalise each patch before MSE (prevents DC-component shortcuts)

training:
  run_name: spark-resnet50
  batch_size: 64
  lr: 1.5e-4               # AdamW with cosine schedule
  weight_decay: 5.0e-2
  epochs: 200
  warmup_epochs: 20
  grad_clip: 3.0
  checkpoint_every: 20
  output_dir: outputs/spark

data:
  image_size: 224
  num_workers: 5
  cache_in_ram: true
  datasets:
    - name: nih
      root_dir: datasets/nih-chest-xrays
      txt_file: datasets/nih-chest-xrays/test.txt   # test split (~11k) for local runs

augmentations:
  random_crop_scale: [0.2, 1.0]  # lighter than contrastive methods — masking is the hard task
  rotation_degrees: 10
  color_jitter_prob: 0.8
  color_jitter_brightness: 0.8
  color_jitter_contrast: 0.8
  gaussian_noise_std: 0.05       # lighter noise — reconstruction needs a cleaner signal
